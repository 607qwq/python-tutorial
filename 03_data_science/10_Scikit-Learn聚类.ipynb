{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b88fc82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e755585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "               [4, 2], [4, 4], [4, 0]])\n",
    "kmeans = KMeans(n_clusters=2,random_state=10).fit(X)\n",
    "print(kmeans.labels_)\n",
    "# array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
    "print(kmeans.predict([[0, 0], [5, 4]]))\n",
    "# array([0, 1], dtype=int32)\n",
    "print(kmeans.cluster_centers_)\n",
    "# array([[1., 2.], [4., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31893f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "# X为样本特征，Y为样本簇类别， 共1000个样本，每个样本4个特征，共4个簇，簇中心在[-1,-1], [0,0],[1,1], [2,2]， 簇方差分别为[0.4, 0.2, 0.2]\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1,-1], [0,0], [1,1], [2,2]], cluster_std=[0.4, 0.2, 0.2, 0.2], \n",
    "                  random_state =9)\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "y_pred = KMeans(n_clusters=2, random_state=9).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.calinski_harabaz_score(X, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a24629",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = KMeans(n_clusters=4, random_state=9).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea3821",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.calinski_harabaz_score(X, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48db68",
   "metadata": {},
   "source": [
    "# MiniBatchKMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "y_pred = MiniBatchKMeans(n_clusters=2, batch_size = 200, random_state=9).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.calinski_harabaz_score(X, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MiniBatchKMeans(n_clusters=3, batch_size = 200, random_state=9).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "metrics.calinski_harabaz_score(X, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MiniBatchKMeans(n_clusters=4, batch_size = 200, random_state=9).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "metrics.calinski_harabaz_score(X, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18641afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots_adjust(left=.02, right=.98, bottom=.096, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "plt.subplot(2,2,1)\n",
    "y_pred = MiniBatchKMeans(n_clusters=2, batch_size = 200, random_state=9).fit_predict(X)\n",
    "score2= metrics.calinski_harabaz_score(X, y_pred)  \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.text(.99, .01, ('k=%d, score: %.2f' % (2,score2)),\n",
    "                 transform=plt.gca().transAxes, size=10,\n",
    "                 horizontalalignment='right')\n",
    "\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "y_pred = MiniBatchKMeans(n_clusters=3, batch_size = 200, random_state=9).fit_predict(X)\n",
    "score3= metrics.calinski_harabaz_score(X, y_pred)  \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.text(.99, .01, ('k=%d, score: %.2f' % (3,score3)),\n",
    "                 transform=plt.gca().transAxes, size=10,\n",
    "                 horizontalalignment='right')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "y_pred = MiniBatchKMeans(n_clusters=4, batch_size = 200, random_state=9).fit_predict(X)\n",
    "score4= metrics.calinski_harabaz_score(X, y_pred)  \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.text(.99, .01, ('k=%d, score: %.2f' % (4,score4)),\n",
    "                 transform=plt.gca().transAxes, size=10,\n",
    "                 horizontalalignment='right')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "y_pred = MiniBatchKMeans(n_clusters=5, batch_size = 200, random_state=9).fit_predict(X)\n",
    "score5 = metrics.calinski_harabaz_score(X, y_pred)  \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.text(.99, .01, ('k=%d, score: %.2f' % (5,score5)),\n",
    "                 transform=plt.gca().transAxes, size=10,\n",
    "                 horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02030c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots_adjust(left=.02, right=.98, bottom=.096, top=.96, wspace=.1,\n",
    "                    hspace=.1)\n",
    "for index, k in enumerate((2,3,4,5)):\n",
    "    plt.subplot(2,2,index+1)\n",
    "    y_pred = MiniBatchKMeans(n_clusters=k, batch_size = 200, random_state=9).fit_predict(X)\n",
    "    score= metrics.calinski_harabaz_score(X, y_pred)  \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "    plt.text(.99, .01, ('k=%d, score: %.2f' % (k,score)),\n",
    "                 transform=plt.gca().transAxes, size=10,\n",
    "                 horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c56891",
   "metadata": {},
   "source": [
    "# K-means Clustering\n",
    "\n",
    "\n",
    "The plots display firstly what a K-means algorithm would yield\n",
    "using three clusters. It is then shown what the effect of a bad\n",
    "initialization is on the classification process:\n",
    "By setting n_init to only 1 (default is 10), the amount of\n",
    "times that the algorithm will be run with different centroid\n",
    "seeds is reduced.\n",
    "The next plot displays what using eight clusters would deliver\n",
    "and finally the ground truth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Though the following import is not directly being used, it is required\n",
    "# for 3D projection to work\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "estimators = [('k_means_iris_8', KMeans(n_clusters=8)),\n",
    "              ('k_means_iris_3', KMeans(n_clusters=3)),\n",
    "              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,\n",
    "                                               init='random'))]\n",
    "\n",
    "fignum = 1\n",
    "titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']\n",
    "for name, est in estimators:\n",
    "    fig = plt.figure(fignum, figsize=(4, 3))\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n",
    "               c=labels.astype(np.float), edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Petal width')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    ax.set_zlabel('Petal length')\n",
    "    ax.set_title(titles[fignum - 1])\n",
    "    ax.dist = 12\n",
    "    fignum = fignum + 1\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(fignum, figsize=(4, 3))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean(),\n",
    "              X[y == label, 2].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.dist = 12\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb76b9",
   "metadata": {},
   "source": [
    "# Color Quantization using K-Means\n",
    "\n",
    "\n",
    "Performs a pixel-wise Vector Quantization (VQ) of an image of the summer palace\n",
    "(China), reducing the number of colors required to show the image from 96,615\n",
    "unique colors to 64, while preserving the overall appearance quality.\n",
    "\n",
    "In this example, pixels are represented in a 3D-space and K-means is used to\n",
    "find 64 color clusters. In the image processing literature, the codebook\n",
    "obtained from K-means (the cluster centers) is called the color palette. Using\n",
    "a single byte, up to 256 colors can be addressed, whereas an RGB encoding\n",
    "requires 3 bytes per pixel. The GIF file format, for example, uses such a\n",
    "palette.\n",
    "\n",
    "For comparison, a quantized image using a random codebook (colors picked up\n",
    "randomly) is also shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa27464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.utils import shuffle\n",
    "from time import time\n",
    "\n",
    "n_colors = 64\n",
    "\n",
    "# Load the Summer Palace photo\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "\n",
    "# Convert to floats instead of the default 8 bits integer coding. Dividing by\n",
    "# 255 is important so that plt.imshow behaves works well on float data (need to\n",
    "# be in the range [0-1])\n",
    "china = np.array(china, dtype=np.float64) / 255\n",
    "\n",
    "# Load Image and transform to a 2D numpy array.\n",
    "w, h, d = original_shape = tuple(china.shape)\n",
    "assert d == 3\n",
    "image_array = np.reshape(china, (w * h, d))\n",
    "\n",
    "print(\"Fitting model on a small sub-sample of the data\")\n",
    "t0 = time()\n",
    "image_array_sample = shuffle(image_array, random_state=0)[:1000]\n",
    "kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Get labels for all points\n",
    "print(\"Predicting color indices on the full image (k-means)\")\n",
    "t0 = time()\n",
    "labels = kmeans.predict(image_array)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "codebook_random = shuffle(image_array, random_state=0)[:n_colors]\n",
    "print(\"Predicting color indices on the full image (random)\")\n",
    "t0 = time()\n",
    "labels_random = pairwise_distances_argmin(codebook_random,\n",
    "                                          image_array,\n",
    "                                          axis=0)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "def recreate_image(codebook, labels, w, h):\n",
    "    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "    d = codebook.shape[1]\n",
    "    image = np.zeros((w, h, d))\n",
    "    label_idx = 0\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            image[i][j] = codebook[labels[label_idx]]\n",
    "            label_idx += 1\n",
    "    return image\n",
    "\n",
    "# Display all results, alongside original image\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.axis('off')\n",
    "plt.title('Original image (96,615 colors)')\n",
    "plt.imshow(china)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, K-Means)')\n",
    "plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n",
    "\n",
    "plt.figure(3)\n",
    "plt.clf()\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, Random)')\n",
    "plt.imshow(recreate_image(codebook_random, labels_random, w, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b99618",
   "metadata": {},
   "source": [
    "基于轮廓分析的簇数的选择\n",
    "\n",
    "\n",
    "轮廓分析可用于研究所得到的簇之间的分离距离。轮廓图显示了一个集群中的每个点与相邻集群中的点的距离的度量，从而提供了一种直观地评估参数（如集群的数量）的方法。这项措施的范围为[-1, 1 ]。\n",
    "轮廓系数（如这些值所指出的）接近+1表明样本远离相邻的簇。值0表示样本位于或非常接近两个相邻集群之间的决策边界，负值表示那些样本可能被分配到错误的集群。\n",
    "\n",
    "在这个例子中，轮廓分析被用来选择最佳值的N13集群。轮廓曲线图显示，对于给定数据，n值3、5和6是不好的选择，这是由于存在具有低于平均轮廓曲线得分的集群，并且还由于轮廓曲线图的尺寸变化很大。轮廓分析在决定2和4之间更矛盾。\n",
    "\n",
    "此外，从轮廓分析的结果可以看到可视化的聚类大小。当n等于2时，由于将3个子集群分组为一个大集群，集群0的轮廓图尺寸较大。然而，当n等于4时，所有图或多或少具有相似的厚度，因此具有相似的大小，这也可以从右侧的标记散点图得到验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)  # For reproducibility\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989f17c",
   "metadata": {},
   "source": [
    "基于经验的k-means初始化方法\n",
    "\n",
    "评估k-均值初始化的能力，以使算法收敛稳健，如通过聚类惯性的相对标准偏差（即到最近聚类中心的平方距离之和）测量的。\n",
    "\n",
    "第一个图显示了最佳初始化参数（``KMeans`` or ``MiniBatchKMeans``）和init方法（``init=\"random\"`` or ``init=\"kmeans++\"``）的选择。\n",
    "\n",
    "第二个图显示了使用``init=\"random\"`` and ``n_init=1``的``MiniBatchKMeans``一次运行结果。这种运行导致一个坏的收敛（局部最优）。\n",
    "\n",
    "用于评估的数据集是符合高斯分布的2D网格数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "# Number of run (with randomly generated dataset) for each strategy so as\n",
    "# to be able to compute an estimate of the standard deviation\n",
    "n_runs = 5\n",
    "\n",
    "# k-means models can do several random inits so as to be able to trade\n",
    "# CPU time for convergence robustness\n",
    "n_init_range = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Datasets generation parameters\n",
    "n_samples_per_center = 100\n",
    "grid_size = 3\n",
    "scale = 0.1\n",
    "n_clusters = grid_size ** 2\n",
    "\n",
    "\n",
    "def make_data(random_state, n_samples_per_center, grid_size, scale):\n",
    "    random_state = check_random_state(random_state)\n",
    "    centers = np.array([[i, j]\n",
    "                        for i in range(grid_size)\n",
    "                        for j in range(grid_size)])\n",
    "    n_clusters_true, n_features = centers.shape\n",
    "\n",
    "    noise = random_state.normal(\n",
    "        scale=scale, size=(n_samples_per_center, centers.shape[1]))\n",
    "\n",
    "    X = np.concatenate([c + noise for c in centers])\n",
    "    y = np.concatenate([[i] * n_samples_per_center\n",
    "                        for i in range(n_clusters_true)])\n",
    "    return shuffle(X, y, random_state=random_state)\n",
    "\n",
    "# Part 1: Quantitative evaluation of various init methods\n",
    "\n",
    "plt.figure()\n",
    "plots = []\n",
    "legends = []\n",
    "\n",
    "cases = [\n",
    "    (KMeans, 'k-means++', {}),\n",
    "    (KMeans, 'random', {}),\n",
    "    (MiniBatchKMeans, 'k-means++', {'max_no_improvement': 3}),\n",
    "    (MiniBatchKMeans, 'random', {'max_no_improvement': 3, 'init_size': 500}),\n",
    "]\n",
    "\n",
    "for factory, init, params in cases:\n",
    "    print(\"Evaluation of %s with %s init\" % (factory.__name__, init))\n",
    "    inertia = np.empty((len(n_init_range), n_runs))\n",
    "\n",
    "    for run_id in range(n_runs):\n",
    "        X, y = make_data(run_id, n_samples_per_center, grid_size, scale)\n",
    "        for i, n_init in enumerate(n_init_range):\n",
    "            km = factory(n_clusters=n_clusters, init=init, random_state=run_id,\n",
    "                         n_init=n_init, **params).fit(X)\n",
    "            inertia[i, run_id] = km.inertia_\n",
    "    p = plt.errorbar(n_init_range, inertia.mean(axis=1), inertia.std(axis=1))\n",
    "    plots.append(p[0])\n",
    "    legends.append(\"%s with %s init\" % (factory.__name__, init))\n",
    "\n",
    "plt.xlabel('n_init')\n",
    "plt.ylabel('inertia')\n",
    "plt.legend(plots, legends)\n",
    "plt.title(\"Mean inertia for various k-means init across %d runs\" % n_runs)\n",
    "\n",
    "# Part 2: Qualitative visual inspection of the convergence\n",
    "\n",
    "X, y = make_data(random_state, n_samples_per_center, grid_size, scale)\n",
    "km = MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=1,\n",
    "                     random_state=random_state).fit(X)\n",
    "\n",
    "plt.figure()\n",
    "for k in range(n_clusters):\n",
    "    my_members = km.labels_ == k\n",
    "    color = cm.nipy_spectral(float(k) / n_clusters, 1)\n",
    "    plt.plot(X[my_members, 0], X[my_members, 1], 'o', marker='.', c=color)\n",
    "    cluster_center = km.cluster_centers_[k]\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o',\n",
    "             markerfacecolor=color, markeredgecolor='k', markersize=6)\n",
    "    plt.title(\"Example cluster allocation with a single random init\\n\"\n",
    "              \"with MiniBatchKMeans\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4928d9",
   "metadata": {},
   "source": [
    "比较K-Means和MiniBatchKMeans算法\n",
    "\n",
    "结论：初始化一致的情况下，差别很小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2702e",
   "metadata": {},
   "source": [
    "手写体数字数据的k-均值聚类演示\n",
    "在这个例子中，我们比较k-means的各种初始化策略在运行和结果的效果。\n",
    "由于背景真实性是已知的，因此我们还应用不同的聚类质量度量来判断聚类标签对背景真实性的拟合程度。\n",
    "评估聚类质量度量（参见定义和讨论度量的聚类性能评估）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "digits = load_digits()\n",
    "data = scale(digits.data)\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "labels = digits.target\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))\n",
    "\n",
    "\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# in this case the seeding of the centers is deterministic, hence we run the\n",
    "# kmeans algorithm only once with n_init=1\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "print(82 * '_')\n",
    "\n",
    "# #############################################################################\n",
    "# Visualize the results on PCA-reduced data\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b7596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
